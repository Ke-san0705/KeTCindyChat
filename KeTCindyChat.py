# import os
# import json
# import numpy as np
# import faiss
# from dotenv import load_dotenv
# from openai import OpenAI
# from tqdm import tqdm

# # --- ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã‚’èª­ã¿è¾¼ã¿ ---
# load_dotenv()
# client = OpenAI()

# # --- è¨­å®š ---
# EMBED_MODEL = "text-embedding-3-small"
# JSONL_PATH = "tagged_chatbot_data.jsonl"
# TOP_K = 3

# # --- JSONLãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ ---
# entries = []
# with open(JSONL_PATH, "r", encoding="utf-8") as f:
#     for line in f:
#         entries.append(json.loads(line))

# # --- Embeddingç”Ÿæˆ ---
# print("ğŸ”„ ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆä¸­...")
# texts = [entry["content"] for entry in entries]
# embeddings = []

# for i in tqdm(range(0, len(texts), 100)):
#     batch = texts[i:i + 100]
#     res = client.embeddings.create(model=EMBED_MODEL, input=batch)
#     batch_embeddings = [d.embedding for d in res.data]
#     embeddings.extend(batch_embeddings)

# embedding_matrix = np.array(embeddings).astype("float32")

# # --- FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ ---
# index = faiss.IndexFlatL2(len(embedding_matrix[0]))
# index.add(embedding_matrix)

# # --- å¿œç­”ç”Ÿæˆé–¢æ•° ---
# def search_and_respond(query):
#     query_vec = client.embeddings.create(
#         model=EMBED_MODEL,
#         input=[query]
#     ).data[0].embedding

#     D, I = index.search(np.array([query_vec]).astype("float32"), TOP_K)
#     results = [entries[i] for i in I[0]]

#     context = "\n\n".join([f"ã€{r['title']}ã€‘\n{r['content']}" for r in results])

#     response = client.chat.completions.create(
#         model="gpt-4",
#         messages=[
#             {"role": "system", "content": "ã‚ãªãŸã¯KeTCindyã¨ã„ã†æ•°å­¦ã‚½ãƒ•ãƒˆã«è©³ã—ã„å°‚é–€ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
#             {"role": "user", "content": f"{context}\n\nè³ªå•ï¼š{query}"}
#         ],
#         temperature=0.2
#     )
#     return response.choices[0].message.content

# # --- å®Ÿè¡Œ ---
# if __name__ == "__main__":
#     print("KeTCindy ChatBot ã¸ã‚ˆã†ã“ã")
#     while True:
#         user_input = input("è³ªå•ã‚’ã©ã†ãï¼ˆçµ‚äº†=exitï¼‰ï¼š")
#         if user_input.lower() in ["exit", "quit"]:
#             break
#         try:
#             answer = search_and_respond(user_input)
#             print("ğŸ’¡ å›ç­”:", answer, "\n")
#         except Exception as e:
#             print("âš ï¸ ã‚¨ãƒ©ãƒ¼:", str(e))


import os
import json
import numpy as np
import faiss
from dotenv import load_dotenv
from openai import OpenAI
from tqdm import tqdm

# --- ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã‚’èª­ã¿è¾¼ã¿ ---
load_dotenv()
client = OpenAI()

# --- è¨­å®š ---
EMBED_MODEL = "text-embedding-3-small"
JSONL_PATH = "tagged_chatbot_data.jsonl"
TOP_K = 3
REJECT_PATTERNS = ["import ", "def ", "matplotlib", "plt.", "print(", "Python"]

# --- JSONLãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ ---
entries = []
with open(JSONL_PATH, "r", encoding="utf-8") as f:
    for line in f:
        entries.append(json.loads(line))

# --- Embeddingç”Ÿæˆ ---
print("ğŸ”„ ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆä¸­...")
texts = [entry["content"] for entry in entries]
embeddings = []

for i in tqdm(range(0, len(texts), 100)):
    batch = texts[i:i + 100]
    res = client.embeddings.create(model=EMBED_MODEL, input=batch)
    batch_embeddings = [d.embedding for d in res.data]
    embeddings.extend(batch_embeddings)

embedding_matrix = np.array(embeddings).astype("float32")

# --- FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ ---
index = faiss.IndexFlatL2(len(embedding_matrix[0]))
index.add(embedding_matrix)

# --- å¿œç­”ç”Ÿæˆé–¢æ•° ---
def search_and_respond(query):
    query_vec = client.embeddings.create(
        model=EMBED_MODEL,
        input=[query]
    ).data[0].embedding

    D, I = index.search(np.array([query_vec]).astype("float32"), TOP_K)
    results = [entries[i] for i in I[0]]
    context = "\n\n".join([f"ã€{r['title']}ã€‘\n{r['content']}" for r in results])

    system_prompt = (
        "ã‚ãªãŸã¯KeTCindyã¨ã„ã†æ•°å­¦ã‚½ãƒ•ãƒˆã«è©³ã—ã„å°‚é–€ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"
        "å‡ºåŠ›ã™ã‚‹ã‚³ãƒ¼ãƒ‰ã¯å¿…ãšKeTCindyï¼ˆCindyScriptã‚„KeTCindyJSï¼‰ã®æ§‹æ–‡ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚"
        "Pythonã‚„ä»–ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã§å‡ºåŠ›ã—ãªã„ã§ãã ã•ã„ã€‚"
        "KeTCindyã§ã®è¡¨ç¾ãŒé›£ã—ã„å ´åˆã¯ã€ãã®ç†ç”±ã¨è¿‘ã„æ§‹æ–‡ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
    )

    for attempt in range(2):  # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ä»˜ãå†ç”Ÿæˆã‚’1å›ã¾ã§è¨±å®¹
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"{context}\n\nè³ªå•ï¼š{query}"}
            ],
            temperature=0.2
        )
        content = response.choices[0].message.content
        if not any(pat in content for pat in REJECT_PATTERNS):
            return content
    return "âš ï¸ KeTCindyå½¢å¼ã§ã®å‡ºåŠ›ãŒã§ãã¾ã›ã‚“ã§ã—ãŸã€‚è³ªå•ã‚’ã‚‚ã†å°‘ã—å…·ä½“çš„ã«ã—ã¦å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"

# --- å®Ÿè¡Œ ---
if __name__ == "__main__":
    print("KeTCindy ChatBot ã¸ã‚ˆã†ã“ã")
    while True:
        user_input = input("è³ªå•ã‚’ã©ã†ãï¼ˆçµ‚äº†=exitï¼‰ï¼š")
        if user_input.lower() in ["exit", "quit"]:
            break
        try:
            answer = search_and_respond(user_input)
            print("ğŸ’¡ å›ç­”:", answer, "\n")
        except Exception as e:
            print("âš ï¸ ã‚¨ãƒ©ãƒ¼:", str(e))

